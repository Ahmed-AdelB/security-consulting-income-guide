# CODEX AI/ML SECURITY 10K

## Purpose
- Synthesize AI security, ML security, and adversarial ML research across repository sources.
- Consolidate consulting rate benchmarks, market growth signals, and opportunity areas.
- Provide a long-form reference for AI/ML security consulting, red teaming, and governance.
- Highlight cross-sector adversarial ML risks and AI-specific consulting offers.
- Deliver a 500+ line field guide grounded in the repository's 10K research corpus.

## Scope
- AI security consulting for LLMs, ML pipelines, and agentic workflows.
- Adversarial ML threats including poisoning, extraction, jailbreaks, and prompt injection.
- Market growth drivers, pricing, and compensation benchmarks.
- Service menu, delivery process, and documentation expectations.

## Source Coverage
- `CODEX_AI_SECURITY_10K.md`: 10K research report covering threats, services, and pricing.
- `AI_SECURITY_CONSULTING_GUIDE_2025.md`: rates, services, and market insights for AI security.
- `LINKEDIN_SECURITY_CONSULTING_RESEARCH_2025.md`: specialization premiums and AI governance trends.
- `INDEED_CYBERSECURITY_SALARY_COMPILATION_2025.md`: market size, CAGR, and AI security premiums.
- `SECURITY_CONSULTING_SALARY_RESEARCH_2025.md`: AI/ML security salaries and high-demand skills.
- `docs/PART4_RATE_GUIDE.md`: AI/ML security rate potential and premium positioning.
- `docs/PART7_NEW_CATEGORIES.md`: AI/ML security platform category expansion.
- `FREELANCE_PLATFORMS_CYBERSECURITY_COMPREHENSIVE_2025.md`: AI security services and emerging niches.
- `RESEARCH_INDEX_2025.md`: AI/ML security listed as premium specialization.
- `HACKER_NEWS_COMPREHENSIVE_INSIGHTS_2025.md`: AI red teaming career signals and adversarial ML awareness.
- `CODEX_AGRICULTURE_AGTECH_5K.md`: adversarial ML risk in AI-driven agronomy.
- `CODEX_MANUFACTURING_ICS_5K.md`: adversarial AI in OT and industrial safety assessments.

## Executive Summary
- AI/ML security consulting has shifted from niche research to mainstream demand as LLMs move into production.
- The fastest growing demand area is LLM application security and AI red teaming.
- Organizations are scrambling to secure LLMs and ML pipelines against adversarial attacks, leakage, and compliance risk.
- Market growth is driven by product pressure, regulatory exposure, customer trust requirements, and model IP value.
- AI security consulting is treated as a premium service with higher rates than baseline AppSec.
- Hourly rates span $100-$150 for junior, $150-$300 mid-level, $300-$500+ senior, and $500-$1,000+ elite specialist tiers.
- Specialized AI security consultants commonly charge $200-$400/hr, with principal experts at $450-$600/hr.
- AI/ML security rate potential is listed at $400-$700+/hr in premium rate guides.
- AI red teaming specialists are reported at $300-$500/hr for advanced AI testing.
- Fixed-scope LLM audits range from $25k-$100k+; full LLM red team exercises range from $50k-$250k.
- AI governance and risk programs run $80k-$250k; AI security readiness assessments run $20k-$60k.
- Monthly retainers range from $5k-$15k (advisory) to $8k-$40k+ (continuous evaluation) depending on scope.
- AI security specialist salaries range from $90k-$160k, with senior roles $200k+, and emerging roles at $150k-$250k.
- The broader cybersecurity consulting market is projected to grow from $17.10B (2025) to $41.15B (2030), CAGR 19.2%.
- AI security engineering premiums are estimated at +20-30% today, rising to +40-50% by 2030.
- AI/ML security is repeatedly flagged as a premium specialization in market trend indexes and LinkedIn research.
- Emerging AI/ML security platforms include Mindgard, HiddenLayer, Protect AI, Lakera, Shaip, and Mend AI.
- Cross-sector adversarial ML risks are already noted in agtech and industrial OT contexts.
- Opportunities favor consultants who blend ML expertise, AppSec, data privacy, and governance.

## Market Growth Drivers
- AI adoption is accelerating across software, finance, healthcare, retail, and government.
- LLM features are shipping fast, often without traditional security reviews, to keep pace with competitors.
- Regulatory exposure is rising via EU AI Act, GDPR, and sector regulations.
- Customer trust demands now include AI risk assessments and contractual guarantees.
- Publicized incidents of data leakage and unsafe outputs create reputational risk.
- Model IP value and proprietary datasets elevate theft and extraction concerns.
- Shadow AI discovery and governance gaps are creating new consulting demand.
- Procurement teams increasingly demand third-party assurance for AI features.
- AI safety and adversarial ML research is becoming a standard reference for red teamers.
- Specialist-era signals show generalist consultants facing more competition than AI security specialists.

## Buyer Triggers
- Public launch of a new AI/LLM feature.
- Enterprise security questionnaires or procurement requirements.
- Regulatory audit readiness for AI systems.
- Incident or near-miss involving prompt injection or data leakage.
- M&A or due diligence involving AI products or model IP.
- Vendor risk reviews for third-party model providers.

## Market Growth Signals
- AI/ML security is listed as a hot specialization with premium expectations in 2025 market indexes.
- LinkedIn consulting research frames 2025 as a specialist era with AI security commanding significant premiums.
- Freelance platforms list AI security and compliance as a distinct service category.
- AI security appears alongside cloud security and zero trust as a high-growth consulting niche.
- AI red teaming is described as an emerging field with unique skillsets.
- AI governance and compliance (EU AI Act readiness) drives 20-30% premium pricing.
- AI security engineer premiums are projected to increase 40-50% by 2030.

## Market Growth Metrics (Cybersecurity Consulting Baseline)
| Metric | Value |
| --- | --- |
| 2025 market value | $17.10B |
| 2030 projected value | $41.15B |
| CAGR (2025-2030) | 19.2% |
| High-growth service areas | AI security consulting, zero trust, cloud security, privacy, incident response |

## Workforce Demand Indicators
- 457,398 cybersecurity job openings (CyberSeek indicator).
- 750,000+ unfilled U.S. roles.
- 3.5 million empty positions globally.
- U.S. workforce can fill roughly 83% of available jobs.
- Skills shortage drives premium consulting compensation for scarce AI security expertise.

## Rate Benchmarks

### Hourly Rates by Experience (AI Security Consulting Guide 2025)
| Experience Level | Role Focus | Rate Range (USD/hr) | Notes |
| :--- | :--- | :--- | :--- |
| Junior (0-3 years) | GenAI policy, basic audits | $100-$150 | Often assists senior teams |
| Mid-Level (3-7 years) | ML security, risk assessment | $150-$300 | Core implementation & standard audits |
| Senior (7+ years) | AI strategy, architecture | $300-$500+ | Strategic advisory for leadership |
| Elite Specialist | LLM red teaming, research | $500-$1,000+ | Deep niche expertise, adversarial ML |

### Hourly & Daily Benchmarks (CODEX AI Security 10K)
| Region | Role Level | Typical Hourly Range | Typical Daily Range |
| --- | --- | --- | --- |
| US and Canada | Senior consultant | $200-$350 | $1,600-$2,800 |
| US and Canada | Principal expert | $450-$600 | $3,600-$4,800 |
| UK and Western Europe | Senior consultant | $150-$280 (GBP/EUR) | $1,200-$2,200 |
| Asia Pacific | Senior consultant | $120-$250 | $1,000-$2,000 |
| Global offshore | Senior consultant | $80-$160 | $600-$1,300 |

### Specialized Premium Signals
| Source | Role or Specialization | Premium Signal |
| --- | --- | --- |
| docs/PART4_RATE_GUIDE.md | AI/ML Security | $400-$700+/hr rate potential |
| LINKEDIN_SECURITY_CONSULTING_RESEARCH_2025.md | Advanced AI red teaming | $300-$500/hr specialist rate |
| LINKEDIN_SECURITY_CONSULTING_RESEARCH_2025.md | AI governance & compliance | 20-30% premium uplift |

## Project Pricing & Retainers

### Project Pricing (AI Security Consulting Guide 2025)
| Engagement | Price Range | Notes |
| --- | --- | --- |
| AI security risk assessment | $5k-$25k | Small to mid scope |
| Comprehensive LLM audit | $25k-$100k+ | Deep LLM review |
| Enterprise AI transformation security | $100k-$500k+ | Large-scale program |
| Monthly advisory retainer | $5k-$15k/month | 10-25 hours |

### Fixed-Scope Engagement Benchmarks (CODEX AI Security 10K)
| Engagement Type | Typical Duration | Team Size | Budget Range |
| --- | --- | --- | --- |
| AI security readiness assessment | 2-4 weeks | 2-3 | $20k-$60k |
| LLM application security review | 3-5 weeks | 2-4 | $35k-$90k |
| Full LLM red team exercise | 4-8 weeks | 3-5 | $50k-$250k |
| ML robustness and privacy audit | 4-6 weeks | 2-4 | $40k-$120k |
| MLOps and supply chain review | 3-5 weeks | 2-3 | $30k-$80k |

### Retainer Models (Continuous Evaluation)
- $8k-$20k/month for a small evaluation suite and quarterly review.
- $20k-$40k/month for continuous testing, dashboarding, and remediation support.
- $40k-$100k/month for enterprise programs with multiple models and compliance needs.
- $5k-$15k/month advisory retainers reported in AI security consulting guide.

### Rate Card by Service Type (CODEX AI Security 10K)
| Service | Typical Effort | Typical Price Range | Notes |
| --- | --- | --- | --- |
| AI system inventory + risk tiering | 1-2 weeks | $10k-$30k | Governance starter |
| Threat modeling workshop | 2-4 days | $4k-$20k | Depends on team size |
| LLM prompt & guardrail review | 1-2 weeks | $12k-$35k | Prompt refactor guidance |
| RAG security review | 2-4 weeks | $20k-$60k | Data validation, leakage tests |
| Agentic workflow security review | 2-5 weeks | $25k-$80k | Tool authorization focus |
| Automated safety test suite | 2-6 weeks | $25k-$90k | CI-integrated tests |
| ML pipeline hardening | 2-5 weeks | $20k-$70k | Registry + artifact signing |
| Executive risk report & board brief | 3-7 days | $5k-$25k | Add-on deliverable |

### Pricing Drivers
- Model access and transparency (black-box vs white-box).
- Scope breadth across models, tools, and data sources.
- Data sensitivity and regulated data handling requirements.
- Urgent timelines or launch readiness deadlines.
- Agentic workflows and tool integrations increasing complexity.

## Compensation Benchmarks

### Salary Ranges (AI/ML Security)
| Role | Salary Range | Notes |
| --- | --- | --- |
| AI security specialist | $90k-$160k | Consulting guide benchmark |
| Senior AI security engineer | $200k+ | Consulting guide benchmark |
| AI security specialist (emerging roles) | $150k-$250k | Salary research benchmark |

### Premium Signals
| Signal | Current Premium | 2030 Premium |
| --- | --- | --- |
| AI Security Engineer | +20-30% | +40-50% |

### High-Demand Skills (Salary Research)
- AI/ML security listed as a top 10 high-demand skill for 2025.
- Skills shortage and long time-to-fill (3-6 months) drive compensation premiums.

## Opportunity Map

### LLM Security & Red Teaming Services
- Prompt injection testing and prompt bypass analysis.
- Jailbreak and abuse testing for safety guardrails.
- Indirect prompt injection in RAG pipelines.
- Output validation for PII or sensitive data leakage.
- Tool misuse and privilege escalation scenarios.
- Safety regression test suite design.
- LLM firewall policy design and tuning.

### ML Model Security Services (Non-LLM)
- Adversarial example resilience testing.
- Data poisoning and backdoor detection evaluation.
- Model extraction and IP protection review.
- Membership inference and privacy leakage assessment.
- Model risk assessment with robustness metrics.
- Model drift monitoring and rollback plans.

### ML Pipeline & MLOps Security
- Dataset governance and lineage validation.
- Artifact signing and model registry access control.
- Pipeline integrity and dependency review.
- Supply chain risk assessment for pretrained models.
- Observability and drift monitoring integration.

### Governance, Risk, and Compliance
- AI risk assessments aligned to NIST AI RMF.
- EU AI Act readiness and risk classification.
- Model cards, data sheets, and risk registers.
- AI policy development for GenAI usage.
- Audit evidence collection and compliance mapping.

### Implementation & Engineering Support
- Guardrail implementation for input/output filtering.
- Secure model registry and artifact signing.
- Tool authorization layers outside prompts.
- Monitoring pipelines for prompt and tool activity.
- Secure logging and retention strategy.

### Incident Response & Recovery
- AI incident response plans and tabletop exercises.
- Forensics for prompt injection or model abuse.
- Containment and remediation guidance.
- Retest and verification after fixes.

### Emerging Opportunity Signals
- Shadow AI discovery programs in enterprise environments.
- AI-specific penetration testing offerings.
- AI safety assessments for industrial systems.
- AI governance support bundled with legal counsel.

## Threat Landscape

### Training Data Attacks
- Data poisoning and label flipping to bias model outcomes.
- Trigger phrases or images that cause hidden backdoor behavior.
- Corruption of data collection pipelines to insert adversarial content.
- Data leakage through unprotected dataset storage or logs.

### Model Supply Chain & Artifact Attacks
- Malicious model artifacts with hidden payloads.
- Tampering with model weights in transit or at rest.
- Insecure dependency chains in preprocessing code.
- Weak access controls to model registries or checkpoints.

### Inference-Time and API Attacks
- Model extraction via high-volume querying and distillation.
- Membership inference to detect training data membership.
- Model inversion to reconstruct sensitive features from outputs.
- Denial of service through expensive prompts or long-running generation.

### LLM-Specific Threats
- Prompt injection overriding system instructions or tool policies.
- Jailbreak techniques bypassing safety filters.
- Indirect prompt injection through RAG documents.
- Data exfiltration via coercing retrieval of confidential context.
- Tool abuse triggering unintended actions or access.

### Application & Agent Risks
- Excessive tool permissions leading to privilege escalation.
- Ambiguous natural language instructions triggering unintended actions.
- Insecure chaining or memory storage leaking sensitive data.
- Output injection where model outputs are executed without validation.
- Role confusion between system instructions and user prompts.

### Impact Categories
- Confidentiality loss of secrets, PII, and proprietary prompts.
- Integrity loss via manipulated outputs or poisoned decisioning.
- Availability loss through resource exhaustion.
- Safety and reputational harm from toxic or biased outputs.
- Regulatory exposure for non-compliance.

## Adversarial ML Sector Signals
- AgTech AI-driven agronomy models face data poisoning risks that can reduce yields.
- Manufacturing and OT deployments face adversarial AI and data poisoning risks in anomaly detection.
- Industrial AI safety assessments are positioned as a consulting opportunity.
- AI red teaming career guidance emphasizes adversarial ML research as required learning.
- AI safety and adversarial ML are flagged as ongoing research areas for red teamers.

## LLM Security Focus Areas

### LLM Security Foundations
- LLM security is system-level, including prompts, retrieval, tools, and workflows.
- Most risk comes from orchestration layers, not the base model alone.
- System prompts are part of the program and require explicit governance.

### RAG Security Challenges
- Knowledge base access controls must align with user permissions.
- Malicious documents can poison retrieval ranking or inject prompts.
- Vector embeddings should be treated as sensitive artifacts.
- Data provenance and retrieval logs are critical for auditability.

### Agentic Workflow Risks
- Agents with tool permissions can trigger real-world actions.
- Tool chaining can create silent privilege escalation.
- Output execution without review increases safety risk.
- Tool approval workflows and runtime policy engines are needed.

### Defense-in-Depth Strategies
- Input validation and policy checks before prompt assembly.
- Prompt segmentation and context isolation.
- Retrieval safeguards with document access control.
- Least-privilege tool access with explicit allowlists.
- Output filtering for unsafe or disallowed content.
- Rate limiting and query anomaly detection.

### LLM Firewall and Policy Enforcement
- LLM firewalls use rules, classifiers, and guardrail models to inspect inputs/outputs.
- Firewalls are insufficient alone and must be paired with monitoring and human review.
- Policy templates should be versioned and audited.

### Evaluation and Red Teaming for LLMs
- Curated prompt suites and adversarial prompts test safety boundaries.
- Measures include jailbreak success rate and data leakage rate.
- Evaluation should run across model versions and system prompt variants.

### Monitoring and Incident Response
- Logs should capture prompts, outputs, tool calls, and metadata.
- Alerts should trigger on repeated policy violations or abnormal query volume.
- Incident response plans must include containment and model rollback.

### Multimodal and Embedded Use Cases
- Adversarial images can trigger unsafe responses.
- Audio inputs may carry hidden instructions.
- Edge deployments expand attack surface to device logs and physical access.

## ML Model Security Deep Dive

### Data Poisoning and Backdoors
- Poisoning manipulates training data to bias outcomes.
- Backdoors insert hidden triggers causing malicious outputs.
- Detection requires data lineage and validation checks.

### Adversarial Examples
- Small perturbations can cause misclassification in vision or tabular models.
- Risks are acute in safety-critical domains.

### Model Extraction and IP Theft
- High-value models can be copied via repeated queries.
- Extraction is easier when outputs include confidence scores.
- Rate limiting and output sanitization reduce exposure.

### Membership Inference and Model Inversion
- Membership inference reveals whether data was in training sets.
- Model inversion reconstructs sensitive features from outputs.
- Both create privacy and regulatory exposure.

### Pipeline and Supply Chain Risks
- Weak access control allows modification of datasets or artifacts.
- Dependency chains in preprocessing code can introduce malicious behavior.
- Secure pipelines require versioning, hashing, and access control.

### Defensive Techniques
- Data validation and anomaly detection for training and inference.
- Adversarial training and robustness testing.
- Differential privacy or noise injection for sensitive datasets.
- Watermarking and model fingerprinting to detect extraction.
- Output rounding or confidence reduction to limit inversion.
- Audit logging for model artifacts and pipeline access.

### Operational Controls
- Monitor drift and anomalous input patterns.
- Track performance across risk-relevant slices.
- Maintain rollback plans and retraining procedures.

## Data Governance & Privacy

### Data Governance Foundations
- Data classification for PII, PHI, and regulated content.
- Consent and licensing checks for training and RAG data.
- Data minimization to limit exposure.
- Retention policies for prompts, logs, and outputs.

### Privacy Risks in LLM Systems
- LLMs can memorize and regurgitate training data.
- Prompt and output logs often contain sensitive data.
- Fine-tuning on sensitive data increases leakage risk.

### Governance for Knowledge Bases
- Access checks must occur before retrieval.
- Knowledge bases require auditing for sensitive data.
- Embeddings should be treated as sensitive data.

### Privacy-Preserving Techniques
- Differential privacy to limit memorization.
- Synthetic data generation for high-risk datasets.
- Data anonymization and masking for training and evaluation.
- Retrieval filters to remove sensitive fields before prompt assembly.

### Model Governance Processes
- Model cards, data sheets, and risk registers document intent and limits.
- Each model release should document evaluation results and open risks.
- Decommissioning processes are needed when risk thresholds are exceeded.

### Audit Readiness
- Engagements should produce evidence of data sources and controls.
- Documentation supports compliance and procurement requirements.

## MLOps & Supply Chain Security

### Model and Dataset Supply Chain
- Validate provenance and licensing of pretrained models.
- Integrity checks for downloaded checkpoints.
- Dataset versioning with hashes and signed manifests.
- Third-party dataset validation for poisoning risk.

### Secure MLOps Pipelines
- Treat MLOps as CI/CD with security gates.
- Isolate training environments and restrict access to artifacts.
- Require approvals for dataset or model changes.
- Enforce review of preprocessing and feature engineering code.

### Model SBOM and Artifact Signing
- Model SBOMs list dependencies and data sources.
- Signed artifacts reduce tampering risk.
- SBOMs improve audit readiness and traceability.

### Infrastructure Security
- Harden GPU clusters and model serving endpoints.
- Use network segmentation and patch management.
- Rate limit inference endpoints to prevent extraction.
- Monitor authentication and usage anomalies.

### Observability and Drift Monitoring
- Monitor drift and performance anomalies.
- Integrate security signals into MLOps dashboards.
- Use drift signals as indicators of poisoning or abuse.

### Third-Party Vendor Risk
- Assess vendor data handling and retention policies.
- Review model update policies and audit rights.
- Contract for incident response timelines and evidence access.

## Governance, Risk, and Compliance

### Framework Alignment
- NIST AI RMF for governance, mapping, measurement, and management.
- ISO 23894 for AI risk management guidance.
- ISO 42001 for AI management systems.
- ISO 27001 for information security management.
- ISO 27701 for privacy information management.

### Regulatory Landscape
- EU AI Act risk classification and transparency requirements.
- GDPR for personal data processing and automated decisioning.
- HIPAA, GLBA, and PCI DSS for sector-specific data.
- US state privacy laws and sector guidance.

### Risk Assessment and Documentation
- Documented risk assessments, model cards, and data sheets.
- Risk registers tracking severity, likelihood, and mitigation.
- Evidence-based testing to support audits.

### Legal and Contractual Considerations
- Contracts should cover data handling and retention.
- Vendors should clarify model training data usage.
- Audit rights and incident response clauses are expected.

### Compliance-Oriented Testing
- Tests should generate reproducible evidence.
- Coverage of prompt injection, data leakage, and tool misuse is required.

## Metrics & Security Rates

### LLM Security Rates
- Jailbreak Success Rate (JSR): % of attempts bypassing safety controls.
- Prompt Injection Success Rate (PISR): % of prompts overriding system policy.
- Data Leakage Rate (DLR): % of attempts revealing confidential context.
- Tool Abuse Rate (TAR): % of tests triggering unauthorized tool actions.
- Harmful Output Rate (HOR): % of outputs violating content policy.

### ML Security Rates
- Adversarial Success Rate (ASR): % of adversarial inputs changing predictions.
- Extraction Efficiency Rate (EER): surrogate model accuracy vs target.
- Membership Inference Accuracy (MIA): accuracy of detecting training membership.
- Poisoning Impact Rate (PIR): accuracy or bias shift after poisoning.

### Detection and Response Metrics
- Mean Time to Detect (MTTD) for model incidents.
- Mean Time to Remediate (MTTR) for model incidents.
- Alert precision and evaluation suite coverage.

### Benchmark Guidance
- JSR below 5% for high-risk user-facing assistants.
- DLR below 1% for regulated data systems.
- ASR below 10% for safety-critical models after adversarial training.

### Reporting Practices
- Track metrics per model version and prompt family.
- Visualize trends over time rather than single snapshots.
- Use a scorecard for leadership prioritization.

## Engagement Lifecycle

### Common Engagement Models
- Fixed-scope assessment for launch readiness or compliance.
- Sprint-based red team with time-boxed testing.
- Retainer model for ongoing evaluation and monitoring.
- Embedded consultant working inside the AI team.

### Delivery Process Outline
- Discovery and scoping with objectives and assets.
- Environment setup with access and data approvals.
- Threat modeling to map attack paths.
- Test execution with evidence collection.
- Triage and prioritization by impact and likelihood.
- Remediation support with guidance and fixes.
- Retest and validation with updated security rates.

### Communication Cadence
- Kickoff alignment with stakeholders.
- Weekly status updates and mid-engagement readouts.
- Final executive briefing translating technical risks to business impact.

### Data Handling Expectations
- NDAs and data handling agreements are standard.
- Use anonymized or synthetic data when possible.
- Document retention and deletion policies for logs and artifacts.

## Deliverables and Artifacts

### Core Deliverables
- Executive summary with business impact and risk ratings.
- Technical report with reproducible prompts and evidence.
- Threat model diagrams and attack trees.
- Evaluation suite or test harness for continuous testing.
- Prioritized remediation backlog with owners and timelines.
- Retest report showing updated security rates.

### Optional Artifacts
- Policy templates for system prompts and tool permissions.
- Monitoring dashboards and alert definitions.
- Training sessions for engineering and product teams.
- Governance artifacts such as model cards and risk registers.

## Case Study Patterns
- RAG support assistant leakage via indirect prompt injection from untrusted documents.
- Mitigation: isolate user-generated documents, enforce retrieval access checks, and add guardrails.
- Mitigation: jailbreak success rate dropped from 18% to under 3%, DLR below 1%.
- Credit risk model poisoning shifted decision boundaries and exposed fairness risk.
- Mitigation: stricter data lineage controls, validation checks, and API rate limiting.
- Internal code assistant logs exposed source code and credentials in analytics buckets.
- Mitigation: reduce log retention, restrict analytics access, and redact secrets.
- Healthcare triage assistant misused scheduling tool and produced disallowed advice.
- Mitigation: explicit tool authorization, human confirmation, and safety policy updates.

## Maturity Roadmap
- Ad hoc: informal reviews and manual testing.
- Basic: initial threat modeling, small prompt evaluation suite, basic logging.
- Managed: repeatable assessments, documented policies, regular red team exercises.
- Measured: continuous evaluation, dashboards, formal incident response.
- Optimized: automated policy enforcement, model SBOM, cross-team governance.

## Capability Buildout Priorities
- Establish AI asset inventory and data lineage documentation.
- Build or adopt a reusable evaluation harness.
- Create security rate baselines and targets for key models.
- Integrate AI security into existing AppSec and risk workflows.
- Invest in continuous monitoring and incident response processes.

## Go-To-Market and Positioning
- AI security is framed as a premium offering due to scarce ML expertise.
- Positioning themes emphasize risk reduction and launch confidence.
- Executive buyers expect compliance readiness evidence and documentation.
- Differentiators include testing across prompts, retrieval, tools, and infrastructure.
- Clear security metrics and remediation guidance drive repeat work.
- Specialist-era signals show premium for AI security, cloud governance, OT/ICS.
- AI security is described as "everything" in LinkedIn discussions.
- Productized services help avoid the hourly billing trap.

## AI/ML Security Platforms and Ecosystem

### AI/ML Security Platforms (Category Expansion)
- Mindgard (DAST-AI testing tools).
- HiddenLayer (AI defense / AutoRTAI).
- Protect AI (AI security platform, huntr bug bounty).
- Lakera (AI defense, Fortune 500 trusted).
- Shaip (LLM red team, human-led testing).
- Mend AI (AI AppSec, end-to-end AI security).

### Frameworks and Taxonomies
- MITRE ATLAS adversarial ML framework.
- OWASP Top 10 for LLM Applications.
- NIST AI Risk Management Framework.

### Adversarial ML Tooling
- IBM Adversarial Robustness Toolbox.
- CleverHans.
- Foolbox.
- TextAttack.
- OpenAttack.
- SecML.

### LLM Evaluation and Guardrail Tooling
- Prompt evaluation frameworks for curated test suites.
- Guardrail frameworks for input/output policy checks.
- Prompt injection test harnesses for indirect prompt attacks.
- Content safety evaluators for toxicity and harmful output.

### Monitoring and Observability
- Structured logging of prompts, outputs, and tool calls.
- Drift detection systems for model performance changes.
- Alerting on policy violations and abnormal query volume.

## Cross-Sector Opportunities

### Finance and Insurance
- Emphasis on model risk management, explainability, and audit trails.
- Compliance with strict data privacy rules.
- AI risk assessments for automated decisioning.

### Healthcare
- High sensitivity to PHI/PII and strict retention requirements.
- Data minimization and auditability for triage assistants.
- AI governance for clinical decision support.

### Government and Defense
- Classified data handling and strict access controls.
- On-prem deployments and supply chain security.

### Retail and E-Commerce
- Customer-facing chatbots with high volume abuse risk.
- Fraud and data leakage testing for AI assistants.

### SaaS and Enterprise Software
- Multi-tenant data access and tool integration risks.
- Contractual security requirements for AI features.

### Manufacturing and OT
- AI in OT systems introduces adversarial AI and data poisoning risks.
- AI safety assessments for industrial systems and anomaly detection.

### Agriculture and AgTech
- AI-driven agronomy faces adversarial ML and data poisoning risks.
- Consulting demand for securing AI decision models in farming.

### Media and Creative
- IP protection, copyright compliance, and brand safety.

## Freelance and Market Entry Signals
- AI security and compliance listed as service offerings on freelance platforms.
- Specialization guidance includes AI security as a high-demand niche.
- Emerging 2025 niches include AI-powered attack defense and AI system security.
- AI security is tagged as a premium specialization in research indexes.
- AI red teaming is described as an emerging field with a distinct skillset.
- Community advice emphasizes learning ML/LLM fundamentals and adversarial ML research.

## Productized Offering Backlog
- AI Security Readiness Sprint: inventory, governance gap analysis, and risk profiling.
- LLM Application Security Review: prompt audit, RAG review, tool access analysis.
- Prompt Injection & Jailbreak Campaign: curated attack suites and remediation.
- RAG Security Deep Dive: data provenance, access control, retrieval integrity checks.
- Agentic Workflow Security Review: tool authorization and chain-of-action analysis.
- ML Robustness & Privacy Audit: adversarial examples, poisoning simulation, extraction tests.
- MLOps Supply Chain Assessment: dataset provenance, artifact signing, SBOM review.
- Continuous Evaluation Retainer: automated safety suites and security rate dashboards.
- AI Governance & Compliance Program: AI Act mapping, model cards, risk registers.
- Shadow AI Discovery & Policy Rollout: inventory of unauthorized AI tools.
- AI Incident Response Playbook: containment, forensics, and rollback procedures.
- Executive Risk Briefing: board-ready AI risk summaries and metrics.

## Discovery Questions
- What is the system architecture (RAG, agentic, fine-tuned, tool-use)?
- How many models and providers are in scope?
- What data classes are involved (PII, PCI, PHI, trade secrets)?
- What tooling is connected (APIs, databases, command execution)?
- What is the deployment footprint (cloud, on-prem, edge)?
- What is the existing security posture (logging, access control, SDLC)?
- What are the timelines and launch deadlines?
- What safety constraints exist (regulated decisioning, high impact use cases)?
- What is the current incident response and rollback capability?
- What monitoring exists for prompts, outputs, and tool calls?

## Appendix: LLM Security Checklist
- System prompt documented and version controlled.
- Prompt templates stored in secure repositories.
- Input validation for user prompts.
- Prompt segmentation between user input and system instructions.
- RAG data source inventory completed.
- Access controls enforced before retrieval.
- Retrieval logs captured for auditability.
- Document integrity checks in knowledge bases.
- Indirect prompt injection tests for RAG documents.
- Tool allowlists defined and enforced.
- Tool permissions aligned to least privilege.
- Tool execution requires explicit authorization.
- Output filtering for policy violations.
- PII redaction in outputs when required.
- Prompt logging with access controls.
- Output logging with retention policy.
- Rate limiting on inference endpoints.
- Abuse detection for repeated injection attempts.
- Evaluation suite versioned and run per model update.
- Jailbreak success rate tracked.
- Data leakage rate tracked.
- Tool abuse rate tracked.
- Guardrail policy tests documented.
- Human review workflow for high risk outputs.
- Incident response plan for model abuse.
- Rollback plan for model updates.
- Monitoring for anomalous query volume.
- Monitoring for prompt injection patterns.
- Staging environment for red team testing.
- Safe harbor rules for testing documented.
- Documentation of model limitations and intended use.
- Policy for RAG data updates and ingestion.
- Segregation of user-generated vs internal data.
- Controls for system prompt leakage.
- Logging redaction for secrets and credentials.
- Access to prompt logs limited by role.
- Output execution gated by policy approval.
- Session memory isolation for multi-tenant systems.
- Multi-tenant separation for SaaS copilots.

## Appendix: ML Security Checklist
- Training data inventory and lineage documented.
- Data validation checks before training.
- Label integrity checks for critical datasets.
- Backdoor detection tests included in evaluation.
- Poisoning simulations conducted for high-risk models.
- Adversarial robustness tests performed.
- Adversarial training strategy documented.
- Differential privacy considered for sensitive datasets.
- Model artifact hashing and integrity checks.
- Model registry access control enforced.
- Artifact signing for model releases.
- Preprocessing and feature pipeline code reviewed.
- Dependency and supply chain review for ML libraries.
- Output rounding or confidence truncation where needed.
- Rate limiting on inference APIs.
- Model extraction tests executed.
- Membership inference tests executed.
- Model inversion tests executed.
- Watermarking or fingerprinting strategy evaluated.
- Logging of training runs and experiment metadata.
- Dataset versioning with hashes and manifests.
- Approval gates for model promotion to production.
- Drift monitoring for input distribution shifts.
- Performance monitoring by risk-relevant slices.
- Rollback plan for model degradation.
- Incident response playbook for ML-specific issues.
- Secure storage of model weights and checkpoints.
- Access control for training environments.
- Backup and disaster recovery for model artifacts.
- SBOM for model dependencies and data sources.
- Third-party dataset vetting for poisoning risk.
- Privacy impact assessment for sensitive models.
- Compliance mapping for regulated decision systems.

## Appendix: Sample Test Cases
- Prompt injection attempt to override system prompt.
- Prompt injection via RAG document ingestion.
- System prompt extraction request disguised as debugging.
- Data exfiltration prompt targeting confidential context.
- Tool misuse attempt to access restricted API.
- Output injection to trigger code or command execution.
- Jailbreak attempt for disallowed content generation.
- Multi-turn prompt chain to bypass safety filters.
- RAG access control bypass test using unauthorized documents.
- Model extraction test using high-volume query strategy.
- Membership inference attack simulation on target dataset.
- Model inversion attempt to reconstruct sensitive features.
- Adversarial example generation on vision model.
- Adversarial example generation on tabular model.
- Data poisoning test with mislabeled training samples.
- Backdoor trigger test with hidden feature patterns.
- Drift scenario to test anomaly detection.
- Denial-of-service test using long prompt payloads.
- Tool chaining scenario to escalate privileges.
- Output safety test for medical or financial advice.

## Appendix: Glossary
- Adversarial example: input crafted to cause model misclassification.
- AI red teaming: adversarial testing of AI systems to uncover weaknesses.
- Backdoor: hidden trigger that causes specific model behavior.
- Data poisoning: manipulation of training data to bias model behavior.
- Jailbreak: prompt or method that bypasses safety controls.
- Membership inference: attack that determines if a record was in training data.
- Model extraction: reproduction of a model via queries to an API.
- Model inversion: reconstruction of sensitive features from outputs.
- Prompt injection: input that overrides system or developer instructions.
- RAG: retrieval-augmented generation using external documents.
- SBOM: software or model bill of materials listing dependencies.
- Tool abuse: unauthorized tool invocation or privilege escalation.
- Shadow AI: unauthorized or unmanaged AI tools in an enterprise.
- Guardrail: policy or model-based control enforcing safety constraints.
- Evaluation suite: curated tests for model safety and security.
- Agentic workflow: AI workflow that chains tool actions autonomously.
- Data leakage rate: metric for how often sensitive data is exposed.
- Jailbreak success rate: metric for bypassing safety controls.
- Prompt injection success rate: metric for overriding system instructions.
- Tool abuse rate: metric for unauthorized tool actions.
