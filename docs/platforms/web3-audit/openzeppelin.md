# OpenZeppelin Web3 Audit Deep Dive

This document is a comprehensive, non-official deep dive into how a Web3 audit can be structured when engaging with OpenZeppelin.
It is written for founders, security leads, protocol engineers, and procurement teams evaluating security partners.
Content focuses on process, team structure, pricing models, client types, and success strategies.
The intent is to be practical, transparent, and action-oriented for audit preparation and execution.
This is not an official OpenZeppelin publication and does not replace direct scoping discussions.

## Table of Contents
- 1. Executive Summary
- 2. OpenZeppelin in the Web3 Security Landscape
- 3. Audit Philosophy and Guiding Principles
- 4. Engagement Lifecycle Overview
- 5. Pre-Engagement and Scoping
- 6. Kickoff and Documentation Intake
- 7. Threat Modeling and Design Review
- 8. Manual Review Workflow
- 9. Automated Analysis and Testing
- 10. Specialized Reviews and Deep Dives
- 11. Issue Triage and Severity Model
- 12. Reporting and Remediation
- 13. Retest and Assurance Outputs
- 14. Team Structure and Roles
- 15. Pricing Models and Commercial Terms
- 16. Client Types and Engagement Profiles
- 17. Success Strategies for Clients
- 18. Operational Metrics and Quality Bar
- 19. Post-Audit Security Program
- 20. Appendix A: Pre-Audit Checklist
- 21. Appendix B: Example Timeline
- 22. Appendix C: FAQ
- 23. Appendix D: Glossary

## 1. Executive Summary
- OpenZeppelin audits emphasize depth, clarity, and practical security outcomes.
- Engagements are structured to prioritize high-risk logic and critical attack paths.
- A typical audit blends manual review, automated tooling, and design analysis.
- The process highlights protocol-specific economics, governance, and upgradeability risks.
- Client collaboration is essential for timely fixes and high-quality reporting.
- The audit report serves as both a technical artifact and a stakeholder communication tool.
- A strong audit outcome depends on readiness, documentation, and test coverage.
- Retests validate fixes and reduce residual risk before deployment.
- Audit success is measured by both vulnerability detection and quality of mitigations.
- Long-term security is supported through continuous monitoring and follow-on reviews.

## 2. OpenZeppelin in the Web3 Security Landscape
- OpenZeppelin is known for Ethereum security research and widely used contract libraries.
- The organization provides audits alongside tooling, monitoring, and advisory services.
- Audits commonly focus on Solidity and EVM-based ecosystems.
- The broader ecosystem includes L2s, bridges, DeFi, NFTs, and governance systems.
- OpenZeppelin engagement styles emphasize rigorous review and clear deliverables.
- Security reviews often align with best practices in OpenZeppelin Contracts usage.
- The audit function is supported by internal research and historical exploit analysis.
- Protocols use audits to improve security posture and reduce risk exposure.
- Audits are typically scoped to production-ready codebases and deployment plans.
- Most engagements involve close coordination with protocol engineering teams.
- Audit quality is tied to reviewer expertise, internal process maturity, and client readiness.
- The OpenZeppelin name is associated with conservative security recommendations.

## 3. Audit Philosophy and Guiding Principles
- Defense in depth is a core principle for smart contract security.
- Manual review is prioritized for logic, access control, and state transitions.
- Automated tools support scale, but do not replace expert judgment.
- Security is treated as a lifecycle, not a single event.
- Findings are reported with clear reproduction steps and context.
- Recommendations aim for practical fixes with minimal business disruption.
- Economic and governance risks are treated as first-class security concerns.
- Transparency and communication are emphasized throughout the engagement.
- The process values clear assumptions and documented threat boundaries.
- Auditors focus on real-world attack paths, not just theoretical vulnerabilities.
- Risk classification is designed to help teams prioritize remediation.

## 4. Engagement Lifecycle Overview
- Intake and scoping align expectations, timelines, and system boundaries.
- Kickoff validates documentation, code freeze, and test readiness.
- Review phases blend manual inspection with automated analysis.
- Issue triage occurs continuously as findings are discovered.
- Reporting summarizes risks, fixes, and residual concerns.
- Retest validates remediation and updates status.
- Follow-on guidance may include monitoring and security operations advice.

## 5. Pre-Engagement and Scoping
- The client submits a codebase summary and architecture overview.
- The audit scope lists repositories, commit hashes, and deployment targets.
- Auditors identify out-of-scope components early to avoid confusion.
- A code freeze window is negotiated to prevent moving targets.
- The team defines assumptions for external dependencies and oracles.
- A scoping call clarifies protocol goals, invariants, and risk tolerance.
- The client outlines planned launch timeline and go-live constraints.
- The scope includes supported networks and upgradeability patterns.
- The audit identifies key assets and critical value flows.
- The client discloses any previous audits and known issues.
- Integration points with other protocols are captured in the scope.
- The audit timeline accounts for client availability for clarifications.
- Scoping includes the expected number of contracts and LoC estimates.
- The team considers the use of proxies, beacons, or diamond patterns.
- The audit sets expectations for test coverage and access to private repos.
- A shared vulnerability disclosure process is defined for critical issues.
- The client confirms tooling access, such as forked chains or testnets.
- Risk boundaries are documented for third-party contracts or libraries.
- The engagement notes any custom cryptography or novel primitives.
- The scope includes supported upgrades or governance flows.
- Scoping determines if formal verification is required for core invariants.
- The client provides architecture diagrams and data flow models.
- The audit plan includes time for remediation and a retest pass.
- Commercial terms are agreed after scope size and complexity are clear.
- The audit start date is locked once code freeze is confirmed.

## 6. Kickoff and Documentation Intake
- Kickoff aligns on communication channels and working cadence.
- A shared tracker is established for findings and status updates.
- The client provides a runbook for local testing and deployment.
- Auditors review documentation for intended behavior and assumptions.
- Architecture diagrams are validated against code reality.
- The team reviews specification documents and invariants.
- Known risks or tradeoffs are documented for transparency.
- A list of privileged roles and permissions is collected.
- The client shares test suites and coverage metrics.
- Dependencies are pinned and verified for consistency.
- The audit confirms the exact commit hash under review.
- Any upcoming code changes are documented as out of scope.
- The team gathers artifacts such as ABIs and deployment scripts.
- A list of governance mechanisms is collected for later review.
- The kickoff clarifies response time expectations for auditor questions.
- The team identifies any emergency pause or circuit breaker design.
- The audit confirms key management and multisig policies.
- The client provides the expected transaction flows for core features.
- The project shares any existing threat model or security assessments.
- The kickoff documents how issues will be escalated if critical.

## 7. Threat Modeling and Design Review
- Threat modeling identifies assets, actors, and attack surfaces.
- Auditors map the trust boundaries between contracts and external systems.
- The team outlines assumptions about off-chain services and oracles.
- Threats are prioritized by likelihood and potential impact.
- The process considers adversaries with capital, MEV, and governance power.
- Design review validates invariants like conservation of value or collateralization.
- System roles are analyzed for privilege escalation risks.
- Upgradeability and governance flows are modeled for malicious proposals.
- Cross-chain messaging assumptions are reviewed for relay safety.
- Token economics are checked for manipulation vectors and incentive traps.
- Dependencies such as price feeds are assessed for failure modes.
- Emergency controls are reviewed for abuse and accidental lockouts.
- The team considers denial-of-service and griefing possibilities.
- Risks tied to block timing, randomness, and ordering are considered.
- The model includes extreme market conditions and liquidations.
- Auditors confirm how the protocol behaves under partial failures.
- The design review flags any mismatch between docs and code behavior.
- Security goals are refined and documented before detailed review.
- The threat model informs the deepest manual review paths.
- Findings from the model guide test priorities and fuzzing targets.

## 8. Manual Review Workflow
- Auditors map the codebase into modules and critical paths.
- Entry points are reviewed for authorization and input validation.
- State transitions are checked for invariants and edge cases.
- Reentrancy and cross-function interactions are analyzed carefully.
- Arithmetic operations are checked for precision and overflow behavior.
- External calls are reviewed for unchecked return values and reentrancy.
- Token transfers are verified for safe handling of non-standard tokens.
- Access control modifiers are reviewed for completeness and coverage.
- Upgradeability patterns are checked for storage collisions and gaps.
- Initialization flows are verified to prevent re-initialization bugs.
- Admin and governance capabilities are reviewed for abuse risk.
- Event emissions are checked for consistency and monitoring value.
- Fee calculations are verified for rounding issues and leakage.
- Pause mechanisms are checked for correct enablement and recovery paths.
- Withdraw flows are reviewed for liquidity and insolvency risk.
- Oracles are checked for stale data handling and price manipulation.
- Integrations are reviewed for assumptions about external contract behavior.
- Gas-heavy loops are checked for denial-of-service potential.
- Ownership transfer flows are checked for safe handoff practices.
- Auditors cross-check specs with code to identify logic drift.
- Critical data structures are reviewed for corruption or misuse.
- Upgrade admin separation is checked for clarity and auditability.
- Guard conditions are tested mentally against extreme inputs.
- Manual review includes line-by-line inspection of critical modules.
- Review notes are captured with rationale and supporting context.

## 9. Automated Analysis and Testing
- Static analysis tools flag common vulnerability patterns.
- Linters enforce style and detect suspicious constructs.
- Slither or similar frameworks identify dangerous operations.
- Symbolic execution explores edge cases and path constraints.
- Property-based testing validates invariants across random inputs.
- Fuzzing is applied to functions with complex branching.
- Coverage analysis identifies untested or under-tested code paths.
- The audit may run Foundry, Hardhat, or Truffle test suites.
- Auditors build custom tests for discovered edge cases.
- Differential testing compares expected and actual behavior.
- Automated tools help prioritize manual review hot spots.
- Tool outputs are triaged to reduce false positives.
- Gas usage tests catch out-of-gas risks in loops.
- Static dependency checks verify known library safety.
- Bytecode-level analysis may be used for deployed contracts.
- Invariant testing focuses on conservation of value or collateral ratios.
- The team validates that test oracles match protocol intentions.
- Custom fuzzers may target encoding, decoding, or signature flows.
- Tests are designed to simulate adversarial ordering and MEV.
- Automated reports are integrated into the audit evidence base.
- Tool configuration is adjusted for chain-specific opcodes.
- Auditors document tool limitations and manual compensations.
- Automated findings are paired with manual confirmation.
- The audit ensures tool results map to scope and code freeze.
- Automated checks are re-run after major client fixes.

## 10. Specialized Reviews and Deep Dives
- Upgradeability review validates proxy storage layout and admin access.
- Governance review assesses proposal execution, timelocks, and vetoes.
- Tokenomics review evaluates mint, burn, and distribution controls.
- Cross-chain review checks message verification and replay protection.
- Bridge review considers validator assumptions and proof systems.
- Oracle review analyzes price feeds, update cadence, and fallback logic.
- Liquidation review validates collateral ratios and liquidation incentives.
- Staking review considers reward distribution and slashing logic.
- MEV review examines opportunities for sandwiching or front running.
- Vault review checks share accounting and rounding behavior.
- AMM review inspects invariant maintenance and pricing formulas.
- Lending review examines interest rate models and reserve usage.
- Perpetuals review checks funding rates and liquidation penalties.
- NFT review examines metadata integrity and mint authorization.
- Multisig review checks signer sets and threshold changes.
- Key management review covers hardware usage and rotation policies.
- Bridge rate limits and circuit breakers are validated.
- Pause systems are reviewed for coverage and safe recovery.
- Time-based controls are checked for timestamp manipulation risk.
- Cryptography review checks signature verification and domain separation.
- Replay protection is verified in all signing flows.
- Economic safety is assessed under extreme volatility scenarios.
- Upgrade governance is reviewed for emergency patches and rollback.
- Meta-transaction handling is reviewed for nonce safety.
- The audit considers integration with L2 sequencing policies.
- Cross-contract invariants are tested for shared state drift.
- Specialized reviews are scoped according to protocol risk profile.

## 11. Issue Triage and Severity Model
- Findings are grouped by severity: critical, high, medium, low, informational.
- Severity considers impact, exploitability, and likelihood.
- Critical issues can lead to total loss or protocol failure.
- High issues enable substantial loss or long-term security damage.
- Medium issues involve partial loss or significant governance impact.
- Low issues include best-practice violations with limited impact.
- Informational items capture clarity gaps or minor improvements.
- Each issue includes a description, impact, and suggested remediation.
- Reproduction steps are provided for verifiability.
- The audit records affected files and functions for traceability.
- Findings are tracked with unique identifiers for retest.
- Client feedback can reclassify severity with justification.
- Duplicates are consolidated to reduce noise.
- Auditors document mitigating factors for context.
- Issues are tagged by category such as access control or arithmetic.
- Root causes are identified to prevent repeat bugs.
- Observations may include gas optimization when tied to security.
- The triage process prioritizes high-impact fixes early.
- Issue statuses include open, fixed, mitigated, or accepted risk.
- Accepted risk decisions are documented with rationale.

## 12. Reporting and Remediation
- The draft report is shared for client review and clarifications.
- Findings are described in clear, actionable language.
- The report includes an executive summary for non-technical stakeholders.
- Each issue contains severity, impact, and remediation guidance.
- The report outlines assumptions and scope boundaries.
- A methodology section documents tools and manual techniques used.
- Remediation guidance includes code patterns and design alternatives.
- The team may provide patch suggestions in follow-on communication.
- Clients respond with fixes, questions, or risk acceptance.
- Auditors review fixes for correctness and potential regressions.
- The report captures any unresolved issues at close.
- A final report is issued after remediation review.
- Optional public disclosure timing is discussed with the client.
- The report can include a summary of test coverage or tooling results.
- Clear references enable engineers to map findings to code quickly.
- A remediation timeline is agreed for critical and high issues.
- The report can include post-audit recommendations for monitoring.
- The final report is a key artifact for launches and investors.

## 13. Retest and Assurance Outputs
- Retest focuses on verifying fixes for identified issues.
- Auditors confirm that fixes do not introduce new risks.
- The retest checks for regressions and side effects.
- A retest summary notes which issues are resolved or remain.
- Additional findings may be reported if discovered during retest.
- Retests are constrained to the updated code diff when possible.
- Assurance outputs may include a final letter or summary statement.
- The retest timeline is shorter than the main audit phase.
- The final status reflects residual risk and accepted items.
- Clients use retest results to inform launch readiness.

## 14. Team Structure and Roles
- Engagements are led by a primary audit lead or security engineer.
- The audit lead coordinates scope, schedule, and client communication.
- Primary auditors perform manual review and design analysis.
- Secondary auditors provide independent review and cross-checks.
- Specialists support cryptography, formal methods, or protocol economics.
- A project manager may handle scheduling, intake, and reporting.
- A security researcher contributes exploit knowledge and tooling guidance.
- A QA or verification role may validate report consistency.
- A client success contact manages milestones and commercial terms.
- Teams are structured to balance protocol complexity and timeline.
- Auditors often rotate for fresh perspective on critical modules.
- A formal verification engineer may be added for invariant proofs.
- Analysts may support by building targeted test harnesses.
- Security leads review final findings for consistency and risk framing.
- Cross-team review ensures adherence to audit quality standards.
- Auditors maintain checklists to avoid missing common vulnerability classes.
- The team maintains an internal knowledge base of historical issues.
- Peer review ensures findings are reproducible and well-documented.
- Team structure scales with codebase size and protocol risk.
- An escalation path is defined for urgent, critical discoveries.
- The team may include language specialists for Rust, Move, or Cairo.
- Crypto engineering expertise supports signature and proof system checks.
- Economic analysis is supported by protocol designers or analysts.
- Auditors coordinate to avoid duplicated work and ensure coverage.
- The audit lead compiles final narrative and executive summary.
- Engineers may provide remediation reviews in real time.
- Team members follow standard operating procedures for issue tracking.
- The client team provides counterpart engineers for rapid feedback.
- Collaborative channels include chat, ticketing, and scheduled calls.
- The team logs assumptions and potential blind spots explicitly.

## 15. Pricing Models and Commercial Terms
- Pricing depends on scope, complexity, and timeline constraints.
- Common models include fixed-fee, time-and-materials, and retainers.
- Fixed-fee engagements are common for clear, stable scopes.
- Time-and-materials can fit evolving codebases or research-heavy work.
- Retainer models support continuous auditing or multiple releases.
- Some engagements use milestone-based billing tied to phases.
- Pricing factors include LoC, contract count, and protocol novelty.
- Specialized reviews increase cost due to expertise requirements.
- Rush timelines can add premium fees or require larger teams.
- Extended remediation support may be priced as a separate phase.
- Pricing considers the number of chains and deployments.
- Audit complexity rises with upgradeability and governance features.
- High economic risk protocols often require longer review windows.
- Prior audits can reduce cost if code reuse is verified.
- Open-source dependencies are considered for external risk assessment.
- On-chain integrations increase review time for dependency analysis.
- Client readiness can reduce hours lost to clarifications.
- A code freeze reduces the cost of repeated context switching.
- Pricing may include a retest or offer it as an add-on.
- Some clients negotiate bundle pricing for audits plus monitoring.
- Payment schedules vary by phase or milestone completion.
- Commercial terms define confidentiality and disclosure timelines.
- Statement of work includes scope, deliverables, and assumptions.
- Contract language specifies liability limits and responsibilities.
- Extra support for incident response may be separate.
- The audit budget should include time for internal remediation.
- Teams should reserve time for bug fixes and retest windows.
- Procurement should ask for scope clarity to avoid change orders.
- Pricing conversations benefit from early sharing of architecture docs.
- Expect higher costs for cross-chain messaging or novel cryptography.
- Expect lower cost for minimal-diff reviews of known patterns.
- Pricing does not guarantee absence of vulnerabilities post-launch.

## 16. Client Types and Engagement Profiles
- DeFi protocols seek audits for lending, AMMs, and derivatives.
- L2 and rollup teams require deep protocol and bridge analysis.
- Token projects audit minting logic, vesting, and distribution controls.
- DAOs audit governance, timelocks, and treasury management.
- NFT platforms audit minting, royalties, and metadata integrity.
- Bridges and interoperability layers require cross-chain expertise.
- Wallets and custody solutions audit key management and signing flows.
- Gaming projects audit asset issuance and on-chain marketplaces.
- Infrastructure tools audit upgradeability and admin controls.
- Enterprise teams audit permissioned deployments and compliance controls.
- Labs and research teams audit experimental cryptographic systems.
- Treasury protocols audit asset allocation and rebalancing logic.
- Oracles and data providers audit update flows and resilience.
- Staking services audit validator selection and reward calculation.
- Insurance protocols audit claims and payout logic.
- Launchpads audit token sale logic and vesting contracts.
- Client profiles range from early-stage startups to mature protocols.
- Some clients seek ongoing coverage for multi-release roadmaps.
- Others seek a one-time review before a critical launch.
- Security maturity varies, requiring tailored engagement approaches.
- Clients with robust internal testing benefit from deeper audit focus.
- New teams often need more guidance on security best practices.
- Public protocols often request transparent disclosure timelines.
- Private chains may require NDA-heavy engagement workflows.
- Client engagement style may be asynchronous or high-touch.
- Success depends on aligning audit scope with business priorities.

## 17. Success Strategies for Clients
- Start early with a security roadmap aligned to product milestones.
- Freeze code during audit windows to avoid scope churn.
- Provide clear architecture diagrams and design rationale.
- Document invariants and economic assumptions explicitly.
- Supply comprehensive tests and expected behaviors.
- Provide deployment scripts and configuration for reproducibility.
- Appoint a dedicated engineer to respond quickly to audit questions.
- Track findings in a shared system for transparent status updates.
- Prioritize critical fixes before the retest window.
- Avoid last-minute feature changes during audit execution.
- Validate that external dependencies are audited and pinned.
- Use formal specs or pseudocode for complex mechanisms.
- Ensure admin and emergency controls are clearly documented.
- Provide a realistic remediation timeline and staffing plan.
- Address low-severity issues where feasible to improve quality.
- Conduct internal review before external audit to reduce noise.
- Use audits as learning opportunities to improve engineering standards.
- Align governance timelines with audit and retest phases.
- Clarify launch assumptions such as upgrade pauses or guardian roles.
- Share known tradeoffs so auditors can contextualize findings.
- Encourage auditors to challenge assumptions and invariants.
- Validate testnets or staging environments for audit reproduction.
- Prepare a disclosure strategy for post-audit communications.
- Consider a bug bounty after audit to widen attack surface review.
- Re-audit major changes or high-risk new features.
- Treat audit findings as part of risk management, not a checklist.
- Invest in monitoring and alerting for post-launch incidents.
- Plan incident response playbooks before launch.
- Use audit results to refine user education and documentation.
- Track remediation metrics and lessons learned for future releases.
- Engage stakeholders early to avoid launch delays from security issues.
- Ensure governance key holders understand their operational duties.
- Secure admin keys with hardware devices and multi-signature policies.
- Review privilege boundaries and least-privilege assignments.
- Confirm economic parameters with simulations and stress tests.
- Schedule a pre-audit walkthrough to align on expectations.
- Keep change logs and commit notes to help auditors trace logic.
- Flag any post-audit changes for expedited review.
- Aim for clear, consistent code style to reduce review friction.
- Use modular design to isolate riskier components.
- Include validation for external call return values.
- Use safe math and precision libraries consistently.
- Validate role changes with multi-step confirmations.
- Include time delays for sensitive configuration changes.
- Provide a clear list of events emitted for monitoring.
- Validate that tests cover edge cases and failure paths.
- Confirm that emergency pause does not lock critical funds.
- Ensure treasury workflows have multi-layer approvals.
- Plan for dependency updates and third-party failures.
- Document governance upgrade paths and rollback procedures.
- Capture assumptions about L2 sequencer behavior and reorgs.
- Use audit results to educate community and improve transparency.
- Treat the audit report as a living artifact for ongoing reviews.

## 18. Operational Metrics and Quality Bar
- Audit success is evaluated by the clarity and actionability of findings.
- High-quality reports minimize ambiguity and maximize reproducibility.
- Internal review reduces false positives and inconsistent severity.
- Coverage metrics inform confidence but do not guarantee safety.
- Timeliness is measured against agreed milestone dates.
- Client response time influences throughput and overall audit pace.
- Issue remediation rate reflects effective collaboration.
- Retest closure rate indicates readiness for launch.
- Quality bar includes adherence to internal checklists and standards.
- Audit quality is improved by ongoing research and post-mortems.
- The team tracks common bug patterns across engagements.
- Metrics guide staffing for future audits and complexity estimation.
- Client feedback informs improvements to process and documentation.
- Security results are contextualized by protocol risk profile.
- Quality includes respectful handling of sensitive code and data.
- The team documents any limitations encountered during review.
- Continuous improvement is part of security maturity.

## 19. Post-Audit Security Program
- Post-audit monitoring can detect anomalous transactions and exploits.
- OpenZeppelin Defender and similar tools provide alerts and automation.
- Governance operations can be monitored for unexpected changes.
- Continuous review can be scheduled for significant releases.
- Bug bounties incentivize external review after launch.
- Incident response plans guide actions during critical events.
- Post-audit reviews update threat models as the protocol evolves.
- Security education helps teams avoid repeat mistakes.
- Defensive measures include rate limits, circuit breakers, and caps.
- Key management audits ensure operational safety after launch.
- Post-mortem analysis improves future designs and security posture.
- Security dashboards help stakeholders track risk exposure.
- Community transparency increases trust and accountability.
- Continuous audits can reduce the impact of large feature updates.
- Security tooling integrates with CI pipelines for early warnings.
- Post-audit programs often include training or code review services.
- Protocol upgrades should include mini-audits or diff reviews.
- Emergency processes should be tested in staging environments.
- Security roles are updated as teams scale and governance evolves.
- Monitoring rules should track critical events and parameter changes.
- Clients should revisit risk assumptions after market shifts.
- Secure upgrade processes protect users from rushed changes.
- Post-audit activities protect brand reputation and user funds.
- Regular security reviews build a culture of responsibility.

## 20. Appendix A: Pre-Audit Checklist
- Provide a final audit scope with repositories and commit hashes.
- Confirm a code freeze window and release candidate tag.
- Share architecture diagrams and data flow explanations.
- Provide a list of key invariants and security assumptions.
- Supply complete test suites with instructions to run locally.
- Provide deployment scripts and configuration files.
- Document admin roles, multisig wallets, and key policies.
- Provide descriptions of oracles and external dependencies.
- List all upgradeability patterns used and admin roles.
- Provide a list of supported networks and deployments.
- Document tokenomics and protocol parameters with rationale.
- Provide details of cross-chain communication and relayers.
- Document pausable mechanisms and emergency procedures.
- Share any previous audit reports and known issues.
- Provide a threat model if one exists.
- Outline any time-sensitive launch milestones.
- List third-party libraries and their versions.
- Document integration contracts and external calls.
- Provide access to private repositories or submodules.
- Clarify any out-of-scope components explicitly.
- Provide testnet endpoints or local fork instructions.
- Document randomness sources and entropy assumptions.
- Describe governance flow and timelock parameters.
- List critical events emitted for monitoring.
- Document any custom cryptography or signature schemes.
- Provide a primary and secondary engineering contact.
- Define a shared tracker for findings and remediation status.
- Share expected behavior for edge cases.
- Provide product requirements and business rules.
- Confirm if formal verification is desired for any module.
- Confirm if public disclosure or marketing statements are planned.
- Align on report format and delivery timeline.
- Identify any compliance or regulatory constraints.
- Provide a list of known limitations or accepted risks.
- Share any mitigation already in place for known issues.
- Provide access to staging environments if relevant.
- Ensure code is formatted and linted consistently.
- Confirm dependencies are pinned and reproducible.
- Prepare a remediation plan and expected staffing levels.
- Validate that code comments match actual behavior.

## 21. Appendix B: Example Timeline
- Day 1: Kickoff call, scope confirmation, repo access.
- Day 2: Documentation review and threat model alignment.
- Day 3: Manual review of core contracts begins.
- Day 4: Automated tooling run and triage of findings.
- Day 5: Manual review of admin and governance flows.
- Day 6: Review of integrations, oracles, and external calls.
- Day 7: Fuzzing and property-based tests for invariants.
- Day 8: Draft findings shared, clarify questions with client.
- Day 9: Client remediation work and auditor feedback.
- Day 10: Retest of fixes and final report preparation.
- Day 11: Report delivery and debrief call.
- Day 12: Optional follow-up for unresolved items.

## 22. Appendix C: FAQ
- Q: Is an audit a guarantee of security?
- A: No, audits reduce risk but cannot eliminate it entirely.
- Q: How early should we book an audit?
- A: As early as possible, ideally before the code freeze.
- Q: Can the audit cover multiple repositories?
- A: Yes, but scope must be agreed and time adjusted.
- Q: Can we change code during the audit?
- A: Minor fixes are fine, but major changes should be deferred.
- Q: Do audits include testing and tooling?
- A: Typically yes, but the exact tooling varies by engagement.
- Q: Is a retest included by default?
- A: Often included or offered as a separate phase.
- Q: Can findings be made public?
- A: Disclosure is client-controlled, but transparency is encouraged.
- Q: What languages are supported?
- A: Primarily Solidity, with support for other ecosystems as needed.
- Q: How long does an audit take?
- A: It depends on scope, complexity, and client readiness.
- Q: Should we run a bug bounty after the audit?
- A: Yes, it can complement audits and improve coverage.

## 23. Appendix D: Glossary
- Access control: Mechanisms that restrict who can call functions.
- Admin key: A privileged key that can change protocol settings.
- AMM: Automated market maker, a liquidity pool-based exchange model.
- Attack surface: The set of interfaces an attacker can target.
- Audit scope: The list of code and systems included in the review.
- Bridge: A system that moves assets or messages across chains.
- Circuit breaker: A pause mechanism to prevent further damage.
- Code freeze: A period where the reviewed code does not change.
- Collateralization: The backing of assets by pledged collateral.
- Cross-chain message: A message passed between different networks.
- Denial of service: An attack that makes a system unusable.
- Dependency risk: Risk introduced by third-party code or services.
- Economic attack: Exploits that manipulate incentives or pricing.
- Formal verification: Mathematical proofs about code behavior.
- Fuzzing: Randomized testing to explore edge cases.
- Governance: The process for managing protocol changes.
- Guardian: A role with emergency or limited privileges.
- Invariant: A property that should always hold true.
- Launch readiness: A state where security risk is acceptable.
- LoC: Lines of code, a rough measure of size.
- MEV: Maximal extractable value from ordering transactions.
- Oracle: A service that provides external data on-chain.
- Proxy: A contract pattern that enables upgrades.
- Regression: A bug introduced by a new change.
- Remediation: Fixing issues identified in the audit.
- Retest: A follow-up review of fixes after remediation.
- Scope change: An adjustment to the audit boundaries.
- Severity: A ranking of issue impact and exploitability.
- Slashing: Penalties for validator misbehavior.
- Threat model: A structured view of potential attackers.
- Timelock: A delay before governance changes take effect.
- Upgradeability: The ability to change contract logic post-deploy.
- Vesting: A schedule for releasing tokens over time.
